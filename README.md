## ETL Data Processing Project (Python + MySQL + Airflow)

A project implementing a basic Extract, Transform, Load (ETL) pipeline. The pipeline uses a Python script to perform the data processing logic, a Bash wrapper for execution, and an SQL file to define and populate the target database schema.

Every 5 min whatever the data is updated into the database that data is exceeds the restriction limit then push into the csv file.


## ğŸš€ Features

    ğŸ ETL Logic: Uses a Python script (etl_script.py) to handle data extraction, transformation, and loading.

    ğŸ›¢ï¸ Database Setup: Provides SQL statements in database.txt to initialize the etl_example database and sample_data table.

    ğŸš Wrapper Script: Utilizes wrapper_script.sh to execute the Python ETL script.

    âš™ï¸ Orchestration Ready: Includes etl_dag.py, suggesting readiness for deployment in an orchestrator like Apache Airflow.

## ğŸ› ï¸ Tech Stack

    Core Logic: Python 3

    Database: MySQL

    Execution: Bash Shell Scripting

    Scheduler : Apache Airflow 

## ğŸ“¦ Project Structure

etl-data-processor/

â”œâ”€â”€ wrapper_script.sh # Bash wrapper to execute the ETL script

â”œâ”€â”€ etl_script.py     # Main Python script for E-T-L logic

â”œâ”€â”€ etl_dag.py        # DAG file (likely for Airflow Scheduler)

â”œâ”€â”€ database.txt      # SQL DDL/DML for database setup and initial data

â””â”€â”€ README.md         # Project documentation (this file)

